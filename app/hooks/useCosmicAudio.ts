"use client";

import { useRef, useCallback, useState, useEffect } from "react";

interface UseCosmicAudioReturn {
  /** æ’­æ”¾å®‡å®™å›å£°éŸ³é¢‘ï¼ˆå¸¦ç©ºçµæ··å“æ•ˆæœï¼‰ */
  playCosmicEcho: (base64Audio: string) => Promise<void>;
  /** ç”¨ Web Speech API æœ—è¯»æ–‡æœ¬ */
  speakText: (text: string) => Promise<void>;
  /** æ˜¯å¦æ­£åœ¨æ’­æ”¾ */
  isPlaying: boolean;
  /** æ˜¯å¦æ­£åœ¨åŠ è½½ */
  isLoading: boolean;
  /** é”™è¯¯ä¿¡æ¯ */
  error: string | null;
  /** åˆå§‹åŒ–éŸ³é¢‘å¼•æ“ï¼ˆéœ€åœ¨ç”¨æˆ·äº¤äº’åè°ƒç”¨ï¼‰ */
  initAudio: () => Promise<void>;
  /** æ˜¯å¦å·²åˆå§‹åŒ– */
  isInitialized: boolean;
}

// IR (è„‰å†²å“åº”) é™çº§æ–¹æ¡ˆï¼šåˆæˆä¸€ä¸ªç®€å•çš„æ··å“è„‰å†²å“åº”
function createSyntheticImpulseResponse(
  audioContext: AudioContext,
  duration: number = 4,
  decay: number = 3
): AudioBuffer {
  const sampleRate = audioContext.sampleRate;
  const length = sampleRate * duration;
  const impulseBuffer = audioContext.createBuffer(2, length, sampleRate);

  for (let channel = 0; channel < 2; channel++) {
    const channelData = impulseBuffer.getChannelData(channel);
    for (let i = 0; i < length; i++) {
      // æŒ‡æ•°è¡°å‡çš„ç™½å™ªå£°
      const t = i / sampleRate;
      const envelope = Math.exp(-t * decay);
      // æ·»åŠ ä¸€äº›éšæœºå˜åŒ–ä½¿å…¶æ›´è‡ªç„¶
      const noise = (Math.random() * 2 - 1) * envelope;
      // æ·»åŠ ä¸€äº›æ—©æœŸåå°„
      const earlyReflection =
        i < sampleRate * 0.1 ? Math.random() * 0.3 * Math.exp(-t * 10) : 0;
      channelData[i] = noise * 0.5 + earlyReflection;
    }
  }

  return impulseBuffer;
}

export function useCosmicAudio(): UseCosmicAudioReturn {
  const audioContextRef = useRef<AudioContext | null>(null);
  const convolverRef = useRef<ConvolverNode | null>(null);
  const irLoadedRef = useRef<boolean>(false); // æ ‡è®°æ˜¯å¦å·²åŠ è½½çœŸå® IR
  const [isPlaying, setIsPlaying] = useState(false);
  const [isLoading, setIsLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [isInitialized, setIsInitialized] = useState(false);

  // æ¸…ç†å‡½æ•°
  useEffect(() => {
    return () => {
      if (
        audioContextRef.current &&
        audioContextRef.current.state !== "closed"
      ) {
        audioContextRef.current.close();
      }
    };
  }, []);

  // åå°å¼‚æ­¥åŠ è½½çœŸå® IR æ–‡ä»¶
  const loadRealImpulseResponse = useCallback(async () => {
    if (
      !audioContextRef.current ||
      !convolverRef.current ||
      irLoadedRef.current
    )
      return;

    try {
      const response = await fetch("/impulse-response.wav");
      if (response.ok) {
        const arrayBuffer = await response.arrayBuffer();
        const impulseBuffer = await audioContextRef.current.decodeAudioData(
          arrayBuffer
        );
        convolverRef.current.buffer = impulseBuffer;
        irLoadedRef.current = true;
        console.log("âœ¨ Upgraded to real impulse response (1.51MB loaded)");
      }
    } catch (err) {
      console.log("ğŸ“¡ Using synthetic reverb (IR file load failed):", err);
    }
  }, []);

  // åˆå§‹åŒ– AudioContext å’Œæ··å“
  const initAudio = useCallback(async () => {
    if (isInitialized) return;

    try {
      setIsLoading(true);
      setError(null);

      // åˆ›å»º AudioContext
      const AudioContextClass =
        window.AudioContext ||
        (window as typeof window & { webkitAudioContext: typeof AudioContext })
          .webkitAudioContext;
      const audioContext = new AudioContextClass();
      audioContextRef.current = audioContext;

      // ç¡®ä¿ AudioContext å¤„äºè¿è¡ŒçŠ¶æ€
      if (audioContext.state === "suspended") {
        await audioContext.resume();
      }

      // åˆ›å»º ConvolverNode
      const convolver = audioContext.createConvolver();
      convolverRef.current = convolver;

      // ğŸš€ å¿«é€Ÿå¯åŠ¨ï¼šå…ˆç”¨åˆæˆæ··å“ï¼ˆç¬é—´å®Œæˆï¼‰
      const syntheticBuffer = createSyntheticImpulseResponse(
        audioContext,
        5,
        2
      );
      convolver.buffer = syntheticBuffer;

      setIsInitialized(true);
      console.log("ğŸµ Cosmic Audio Engine initialized (synthetic reverb)");

      // ğŸŒ åå°å¼‚æ­¥åŠ è½½çœŸå® IR æ–‡ä»¶ï¼ˆä¸é˜»å¡ç”¨æˆ·ï¼‰
      loadRealImpulseResponse();
    } catch (err) {
      const message =
        err instanceof Error ? err.message : "Failed to initialize audio";
      setError(message);
      console.error("Audio initialization error:", err);
    } finally {
      setIsLoading(false);
    }
  }, [isInitialized]);

  // æ’­æ”¾å®‡å®™å›å£°
  const playCosmicEcho = useCallback(
    async (base64Audio: string) => {
      if (!audioContextRef.current || !convolverRef.current) {
        // å°è¯•è‡ªåŠ¨åˆå§‹åŒ–
        await initAudio();
        if (!audioContextRef.current || !convolverRef.current) {
          setError("Audio engine not initialized");
          return;
        }
      }

      try {
        setIsLoading(true);
        setError(null);

        const audioContext = audioContextRef.current;

        // ç¡®ä¿ AudioContext å¤„äºè¿è¡ŒçŠ¶æ€
        if (audioContext.state === "suspended") {
          await audioContext.resume();
        }

        // Step 1: è§£ç  Base64 éŸ³é¢‘
        const base64Data = base64Audio.replace(/^data:audio\/\w+;base64,/, "");
        const binaryString = atob(base64Data);
        const bytes = new Uint8Array(binaryString.length);
        for (let i = 0; i < binaryString.length; i++) {
          bytes[i] = binaryString.charCodeAt(i);
        }
        const arrayBuffer = bytes.buffer;

        // Step 2: è§£ç ä¸º AudioBuffer
        const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

        // Step 3: åˆ›å»ºéŸ³é¢‘è·¯ç”±
        //
        // éŸ³é¢‘è·¯ç”±è®¾è®¡ï¼š
        //
        //                    â”Œâ”€> dryGain (0.4) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        // Source â”€â”€> splitterâ”‚                                   â”œâ”€> Destination
        //                    â””â”€> convolver â”€> lowpass â”€> wetGain (0.8) â”€â”˜
        //

        const source = audioContext.createBufferSource();
        source.buffer = audioBuffer;

        // å¹²éŸ³ Gain (ä¿ç•™æ¸…æ™°åº¦)
        const dryGain = audioContext.createGain();
        dryGain.gain.value = 0.4;

        // æ¹¿éŸ³ Gain (æ··å“æ•ˆæœè¦å¼º)
        const wetGain = audioContext.createGain();
        wetGain.gain.value = 0.8;

        // ä½é€šæ»¤æ³¢å™¨ (è®©å£°éŸ³å¬èµ·æ¥æ›´è¿œã€æ›´ç©ºçµ)
        const lowpassFilter = audioContext.createBiquadFilter();
        lowpassFilter.type = "lowpass";
        lowpassFilter.frequency.value = 3500; // åˆ‡æ‰ 3.5kHz ä»¥ä¸Šçš„é«˜é¢‘
        lowpassFilter.Q.value = 0.7;

        // å¯é€‰ï¼šæ·»åŠ ä¸€ç‚¹é«˜é€šæ»¤æ³¢å»æ‰ä½é¢‘éš†éš†å£°
        const highpassFilter = audioContext.createBiquadFilter();
        highpassFilter.type = "highpass";
        highpassFilter.frequency.value = 80;
        highpassFilter.Q.value = 0.5;

        // è¿æ¥å¹²éŸ³è·¯å¾„
        source.connect(dryGain);
        dryGain.connect(audioContext.destination);

        // è¿æ¥æ¹¿éŸ³è·¯å¾„ (æ··å“)
        source.connect(convolverRef.current);
        convolverRef.current.connect(lowpassFilter);
        lowpassFilter.connect(highpassFilter);
        highpassFilter.connect(wetGain);
        wetGain.connect(audioContext.destination);

        // æ’­æ”¾
        setIsPlaying(true);
        source.start(0);

        // ç›‘å¬æ’­æ”¾ç»“æŸ
        source.onended = () => {
          setIsPlaying(false);
          // æ–­å¼€è¿æ¥ä»¥é‡Šæ”¾èµ„æº
          source.disconnect();
          dryGain.disconnect();
          wetGain.disconnect();
          lowpassFilter.disconnect();
          highpassFilter.disconnect();
        };

        setIsLoading(false);
      } catch (err) {
        setIsLoading(false);
        setIsPlaying(false);
        const message =
          err instanceof Error ? err.message : "Failed to play audio";
        setError(message);
        console.error("Audio playback error:", err);
      }
    },
    [initAudio]
  );

  // ä½¿ç”¨ Web Speech API æœ—è¯»æ–‡æœ¬
  const speakText = useCallback(async (text: string) => {
    try {
      setError(null);

      if (!("speechSynthesis" in window)) {
        throw new Error("æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³åˆæˆ");
      }

      // åœæ­¢ä¹‹å‰çš„æœ—è¯»
      window.speechSynthesis.cancel();

      const utterance = new SpeechSynthesisUtterance(text);

      // è®¾ç½®ä¸­æ–‡è¯­éŸ³
      const voices = window.speechSynthesis.getVoices();
      const chineseVoice = voices.find(
        (v) =>
          v.lang.includes("zh") ||
          v.lang.includes("CN") ||
          v.name.includes("Chinese")
      );
      if (chineseVoice) {
        utterance.voice = chineseVoice;
      }

      // è®¾ç½®å‚æ•°
      utterance.rate = 0.85; // ç¨æ…¢ï¼Œæ›´æœ‰æ°›å›´
      utterance.pitch = 0.9; // ç¨ä½æ²‰
      utterance.volume = 0.8;

      setIsPlaying(true);

      utterance.onend = () => {
        setIsPlaying(false);
      };

      utterance.onerror = (event) => {
        setIsPlaying(false);
        setError(`è¯­éŸ³åˆæˆå¤±è´¥: ${event.error}`);
      };

      window.speechSynthesis.speak(utterance);
      console.log("ğŸµ Web Speech API playing:", text);
    } catch (err) {
      setIsPlaying(false);
      const message = err instanceof Error ? err.message : "è¯­éŸ³æ’­æ”¾å¤±è´¥";
      setError(message);
      console.error("Speech API error:", err);
    }
  }, []);

  return {
    playCosmicEcho,
    speakText,
    isPlaying,
    isLoading,
    error,
    initAudio,
    isInitialized,
  };
}
